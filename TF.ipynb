{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzh3GviGuaaO",
        "outputId": "ff6df647-2fe6-409c-bb7f-f857ccc0e7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic 2.2.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.6.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xghEGNOYzYZo",
        "outputId": "7759688d-caef-4ada-f9c8-5b9ef5d48eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppressing tf.hub warnings\n",
        "tf.get_logger().setLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R97sUylL1MuF",
        "outputId": "4238f58b-00d3-4647-b097-833484bb1254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252872794/252872794 [==============================] - 22s 0us/step\n",
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510573713/13510573713 [==============================] - 1052s 0us/step\n",
            "Dataset is downloaded and extracted successfully.\n",
            "Number of images: 82783\n"
          ]
        }
      ],
      "source": [
        "root_dir = \"datasets\"\n",
        "annotations_dir = os.path.join(root_dir, \"annotations\")\n",
        "images_dir = os.path.join(root_dir, \"train2014\")\n",
        "tfrecords_dir = os.path.join(root_dir, \"tfrecords\")\n",
        "annotation_file = os.path.join(annotations_dir, \"captions_train2014.json\")\n",
        "\n",
        "# Download caption annotation files\n",
        "if not os.path.exists(annotations_dir):\n",
        "    annotation_zip = tf.keras.utils.get_file(\n",
        "        \"captions.zip\",\n",
        "        cache_dir=os.path.abspath(\".\"),\n",
        "        origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
        "        extract=True,\n",
        "    )\n",
        "    os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "if not os.path.exists(images_dir):\n",
        "    image_zip = tf.keras.utils.get_file(\n",
        "        \"train2014.zip\",\n",
        "        cache_dir=os.path.abspath(\".\"),\n",
        "        origin=\"http://images.cocodataset.org/zips/train2014.zip\",\n",
        "        extract=True,\n",
        "    )\n",
        "    os.remove(image_zip)\n",
        "\n",
        "print(\"Dataset is downloaded and extracted successfully.\")\n",
        "\n",
        "with open(annotation_file, \"r\") as f:\n",
        "    annotations = json.load(f)[\"annotations\"]\n",
        "\n",
        "image_path_to_caption = collections.defaultdict(list)\n",
        "for element in annotations:\n",
        "    caption = f\"{element['caption'].lower().rstrip('.')}\"\n",
        "    image_path = images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"])\n",
        "    image_path_to_caption[image_path].append(caption)\n",
        "\n",
        "image_paths = list(image_path_to_caption.keys())\n",
        "print(f\"Number of images: {len(image_paths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELHBfuAL29J9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c6461e-b4b7-468e-b3ec-50857cafb67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:33<00:00, 10.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 training examples were written to tfrecord files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:26<00:00,  8.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 evaluation examples were written to tfrecord files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_size = 30000\n",
        "valid_size = 5000\n",
        "captions_per_image = 2\n",
        "images_per_file = 2000\n",
        "\n",
        "train_image_paths = image_paths[:train_size]\n",
        "num_train_files = int(np.ceil(train_size / images_per_file))\n",
        "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
        "\n",
        "valid_image_paths = image_paths[-valid_size:]\n",
        "num_valid_files = int(np.ceil(valid_size / images_per_file))\n",
        "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
        "\n",
        "tf.io.gfile.makedirs(tfrecords_dir)\n",
        "\n",
        "\n",
        "def bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def create_example(image_path, caption):\n",
        "    feature = {\n",
        "        \"caption\": bytes_feature(caption.encode()),\n",
        "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "\n",
        "def write_tfrecords(file_name, image_paths):\n",
        "    caption_list = []\n",
        "    image_path_list = []\n",
        "    for image_path in image_paths:\n",
        "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
        "        caption_list.extend(captions)\n",
        "        image_path_list.extend([image_path] * len(captions))\n",
        "\n",
        "    with tf.io.TFRecordWriter(file_name) as writer:\n",
        "        for example_idx in range(len(image_path_list)):\n",
        "            example = create_example(\n",
        "                image_path_list[example_idx], caption_list[example_idx]\n",
        "            )\n",
        "            writer.write(example.SerializeToString())\n",
        "    return example_idx + 1\n",
        "\n",
        "\n",
        "def write_data(image_paths, num_files, files_prefix):\n",
        "    example_counter = 0\n",
        "    for file_idx in tqdm(range(num_files)):\n",
        "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
        "        start_idx = images_per_file * file_idx\n",
        "        end_idx = start_idx + images_per_file\n",
        "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
        "    return example_counter\n",
        "\n",
        "\n",
        "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
        "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
        "\n",
        "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
        "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLM5xD-x4EtV"
      },
      "outputs": [],
      "source": [
        "feature_description = {\n",
        "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
        "}\n",
        "\n",
        "\n",
        "def read_example(example):\n",
        "    features = tf.io.parse_single_example(example, feature_description)\n",
        "    raw_image = features.pop(\"raw_image\")\n",
        "    features[\"image\"] = tf.image.resize(\n",
        "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
        "    )\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern, batch_size):\n",
        "\n",
        "    return (\n",
        "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
        "        .map(\n",
        "            read_example,\n",
        "            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "            deterministic=False,\n",
        "        )\n",
        "        .shuffle(batch_size * 10)\n",
        "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxOOJy3t4MYt"
      },
      "outputs": [],
      "source": [
        "def project_embeddings(\n",
        "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "):\n",
        "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
        "    for _ in range(num_projection_layers):\n",
        "        x = tf.nn.gelu(projected_embeddings)\n",
        "        x = layers.Dense(projection_dims)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        x = layers.Add()([projected_embeddings, x])\n",
        "        projected_embeddings = layers.LayerNormalization()(x)\n",
        "    return projected_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skmR1Ao_4P6u"
      },
      "outputs": [],
      "source": [
        "def create_vision_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the pre-trained Xception model to be used as the base encoder.\n",
        "    xception = keras.applications.Xception(\n",
        "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    for layer in xception.layers:\n",
        "        layer.trainable = trainable\n",
        "    # Receive the images as inputs.\n",
        "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
        "    # Preprocess the input image.\n",
        "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
        "    # Generate the embeddings for the images using the xception model.\n",
        "    embeddings = xception(xception_input)\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"vision_encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHa6AO-84Tq1"
      },
      "outputs": [],
      "source": [
        "def create_text_encoder(\n",
        "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "):\n",
        "    # Load the BERT preprocessing module.\n",
        "    preprocess = hub.KerasLayer(\n",
        "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n",
        "        name=\"text_preprocessing\",\n",
        "    )\n",
        "    # Load the pre-trained BERT model to be used as the base encoder.\n",
        "    bert = hub.KerasLayer(\n",
        "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
        "        \"bert\",\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    bert.trainable = trainable\n",
        "    # Receive the text as inputs.\n",
        "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
        "    # Preprocess the text.\n",
        "    bert_inputs = preprocess(inputs)\n",
        "    # Generate embeddings for the preprocessed text using the BERT model.\n",
        "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the text encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgh3FgQW4WRv"
      },
      "outputs": [],
      "source": [
        "class DualEncoder(keras.Model):\n",
        "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.text_encoder = text_encoder\n",
        "        self.image_encoder = image_encoder\n",
        "        self.temperature = temperature\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "    def call(self, features, training=False):\n",
        "        # Place each encoder on a separate GPU (if available).\n",
        "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            # Get the embeddings for the captions.\n",
        "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
        "        with tf.device(\"/gpu:1\"):\n",
        "            # Get the embeddings for the images.\n",
        "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
        "        return caption_embeddings, image_embeddings\n",
        "\n",
        "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
        "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
        "        logits = (\n",
        "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
        "            / self.temperature\n",
        "        )\n",
        "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
        "        images_similarity = tf.matmul(\n",
        "            image_embeddings, image_embeddings, transpose_b=True\n",
        "        )\n",
        "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
        "        captions_similarity = tf.matmul(\n",
        "            caption_embeddings, caption_embeddings, transpose_b=True\n",
        "        )\n",
        "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
        "        targets = keras.activations.softmax(\n",
        "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
        "        )\n",
        "        # Compute the loss for the captions using crossentropy\n",
        "        captions_loss = keras.losses.categorical_crossentropy(\n",
        "            y_true=targets, y_pred=logits, from_logits=True\n",
        "        )\n",
        "        # Compute the loss for the images using crossentropy\n",
        "        images_loss = keras.losses.categorical_crossentropy(\n",
        "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
        "        )\n",
        "        # Return the mean of the loss over the batch.\n",
        "        return (captions_loss + images_loss) / 2\n",
        "\n",
        "    def train_step(self, features):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            caption_embeddings, image_embeddings = self(features, training=True)\n",
        "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
        "        # Backward pass\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        # Monitor loss\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, features):\n",
        "        caption_embeddings, image_embeddings = self(features, training=False)\n",
        "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULzs80S44Z89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "bdd43e69-b6b4-4310-8e9d-14a24bf595c4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6d083d16795c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_projection_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m text_encoder = create_text_encoder(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnum_projection_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-11-43fa196d1fd7>\u001b[0m in \u001b[0;36mcreate_text_encoder\u001b[0;34m(num_projection_layers, projection_dims, dropout_rate, trainable)\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Load the pre-trained BERT model to be used as the base encoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     bert = hub.KerasLayer(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_setup_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m_setup_layer\u001b[0;34m(self, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;34m\"\"\"Constructs keras layer with relevant weights and losses.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Initialize an empty layer, then add_weight() etc. as needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# Add trainable and non-trainable weights from the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m             )\n\u001b[1;32m    351\u001b[0m         ):\n\u001b[0;32m--> 352\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    353\u001b[0m                 \u001b[0;34m\"Expected `trainable` argument to be a boolean, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;34mf\"but got: {trainable}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected `trainable` argument to be a boolean, but got: bert"
          ]
        }
      ],
      "source": [
        "num_epochs = 5  # In practice, train for at least 30 epochs\n",
        "batch_size = 256\n",
        "\n",
        "vision_encoder = create_vision_encoder(\n",
        "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
        ")\n",
        "text_encoder = create_text_encoder(\n",
        "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
        ")\n",
        "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
        "dual_encoder.compile(\n",
        "    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
        ")\n",
        "\n",
        "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
        "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
        "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
        "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
        "# Create a learning rate scheduler callback.\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\", factor=0.2, patience=3\n",
        ")\n",
        "# Create an early stopping callback.\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
        ")\n",
        "history = dual_encoder.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[reduce_lr, early_stopping],\n",
        ")\n",
        "print(\"Training completed. Saving vision and text encoders...\")\n",
        "vision_encoder.save(\"vision_encoder\")\n",
        "text_encoder.save(\"text_encoder\")\n",
        "print(\"Models are saved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}